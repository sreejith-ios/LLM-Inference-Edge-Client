import os
import torch
from intel_extension_for_pytorch import ipex
from transformers import AutoModelForCausalLM, AutoTokenizer

class InferenceEngine:
    def __init__(self, model_name_or_path, tiling_module, scheduler):
        """
        Initializes the InferenceEngine. Downloads the model from Hugging Face if it is not available locally.
        
        Args:
        - model_name_or_path (str): The name of the model on Hugging Face or the local path to the model.
        - tiling_module (TilingModule): Handles tiling input data.
        - scheduler (Scheduler): Distributes workloads across available hardware.
        """
        self.model_name_or_path = self._resolve_model_path(model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path)
        
        # Optimize the model using Intel Extension for PyTorch
        self.model = ipex.optimize(self.model, dtype=torch.float16)
        self.tiling_module = tiling_module
        self.scheduler = scheduler

    def _resolve_model_path(self, model_name_or_path):
        """
        Resolves the model path. If the model is not available locally, it is downloaded from Hugging Face.
        
        Args:
        - model_name_or_path (str): The name or path of the model.
        
        Returns:
        - str: The local path to the model.
        """
        if os.path.exists(model_name_or_path):
            print(f"Using locally available model at: {model_name_or_path}")
            return model_name_or_path
        else:
            print(f"Model not found locally. Downloading from Hugging Face: {model_name_or_path}")
            # Downloads the model to the local cache directory
            return model_name_or_path

    def run_inference(self, input_text):
        """
        Runs inference on the input text.

        Args:
        - input_text (str): The input text to process.

        Returns:
        - str: The output text generated by the model.
        """
        # Tokenize input text
        tokens = self.tokenizer(input_text, return_tensors="pt").input_ids
        
        # Partition input tokens into tiles
        tiles = self.tiling_module.partition_input(tokens)
        
        # Process tiles across hardware resources
        outputs = self.scheduler.process_tiles(tiles, self.model)
        
        # Decode and return the output
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
